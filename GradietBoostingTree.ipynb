{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR GOOGLE COLAB\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "#os.environ['USER'] = input('Enter the username of your Github account: ') \n",
    "os.environ['PASSWORD'] = getpass('Enter the password of your Github account: ')\n",
    "#os.environ['REPOSITORY'] = input('Enter the name of the Github repository: ')\n",
    "\n",
    "os.environ['USER'] = 'DimiDR'\n",
    "os.environ['REPOSITORY'] = 'RentPredData'\n",
    "\n",
    "\n",
    "os.environ['GITHUB_AUTH'] = os.environ['USER'] + ':' + os.environ['PASSWORD']\n",
    "\n",
    "!rm -rf $REPOSITORY # To remove the previous clone of the Github repository\n",
    "!git clone https://$GITHUB_AUTH@github.com/$USER/$REPOSITORY.git\n",
    "\n",
    "os.environ['USER'] = os.environ['PASSWORD'] = os.environ['REPOSITORY'] = os.environ['GITHUB_AUTH'] = \"\"\n",
    "! ls\n",
    "%cd RentPredData\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "# https://stackabuse.com/gradient-boosting-classifiers-in-python-with-scikit-learn/\n",
    "# https://blogs.sas.com/content/subconsciousmusings/2017/04/12/machine-learning-algorithm-use/\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rent Shape:  (90637, 37)\n",
      "Buy Shape:  (109, 37)\n"
     ]
    }
   ],
   "source": [
    "cnx_rent = sqlite3.connect('real-estate-rent.db')\n",
    "cnx_buy = sqlite3.connect('real-estate-buy.db')\n",
    "\n",
    "df_rent = pd.read_sql_query(\"SELECT * FROM immoscout\", cnx_rent)\n",
    "df_buy = pd.read_sql_query(\"SELECT * FROM immoscout\", cnx_buy)\n",
    "cnx_rent.close()\n",
    "cnx_buy.close()\n",
    "print('Rent Shape: ', df_rent.shape)\n",
    "print('Buy Shape: ', df_buy.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_for_training(df):\n",
    "    # Prepare the training data for training\n",
    "    # Features:\n",
    "    # Data One Hot Encoding\n",
    "    # will save the encoder files in the folder\n",
    "    # use bag of words for one hot encoding\n",
    "    \n",
    "    #df.immo_id = df.immo_id.astype(str)\n",
    "    \n",
    "    #set to True or False and then to 1 or 0\n",
    "    df[\"balcony\"] = df[\"balcony\"] == \"true\"\n",
    "    df[\"balcony\"] = df.balcony.astype(int)\n",
    "    df[\"cellar\"] = df[\"cellar\"] == \"true\"\n",
    "    df[\"cellar\"] = df.cellar.astype(int)\n",
    "    df[\"garden\"] = df[\"garden\"] == \"true\"\n",
    "    df[\"garden\"] = df.garden.astype(int)\n",
    "    df[\"lift\"] = df[\"lift\"] == \"true\"\n",
    "    df[\"lift\"] = df.lift.astype(int)\n",
    "    \n",
    "    # one hot city encoding\n",
    "    enc_city = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_city.fit(df[['city']])\n",
    "    df_city_enc = pd.DataFrame(data=enc_city.transform(df[['city']]).toarray(), columns=enc_city.get_feature_names(['city']), dtype=bool)\n",
    "    df_city_enc = df_city_enc * 1\n",
    "    # save \n",
    "    joblib.dump(enc_city, 'encoder_city.joblib')\n",
    "\n",
    "    # one hot quarter encoding\n",
    "    enc_quarter = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_quarter.fit(df2[['quarter']])\n",
    "    df_quarter_enc = pd.DataFrame(data=enc_quarter.transform(df[['quarter']]).toarray(), columns=enc_quarter.get_feature_names(['quarter']), dtype=bool)\n",
    "    df_quarter_enc = df_quarter_enc * 1\n",
    "    # save \n",
    "    joblib.dump(enc_quarter, 'encoder_quarter.joblib')\n",
    "\n",
    "    # concatenate training data\n",
    "    X = pd.concat((df[['balcony', 'cellar', 'garden', 'lift', 'livingSpace', 'numberOfRooms']], df_city_enc, df_quarter_enc), axis=1)\n",
    "    y = df[[\"value\"]]\n",
    "    X = bag_of_words(X, df)\n",
    "    return (X, y)\n",
    "\n",
    "def predict_data(df, model, enc_city, enc_quarter):\n",
    "    # predict data with given model\n",
    "    # the formating of the data should be the same as the trained model\n",
    "    \n",
    "    #set to True or False and then to 1 or 0\n",
    "    df[\"balcony\"] = df[\"balcony\"] == \"true\"\n",
    "    df[\"balcony\"] = df.balcony.astype(int)\n",
    "    df[\"cellar\"] = df[\"cellar\"] == \"true\"\n",
    "    df[\"cellar\"] = df.cellar.astype(int)\n",
    "    df[\"garden\"] = df[\"garden\"] == \"true\"\n",
    "    df[\"garden\"] = df.garden.astype(int)\n",
    "    df[\"lift\"] = df[\"lift\"] == \"true\"\n",
    "    df[\"lift\"] = df.lift.astype(int)\n",
    "    \n",
    "    # enc_city, enc_quarter\n",
    "    hot_city = pd.DataFrame(data=enc_city.transform(df[['city']]).toarray(), columns=enc_city.get_feature_names(['city']), dtype=bool)\n",
    "    hot_city = hot_city * 1\n",
    "    hot_quarter = pd.DataFrame(data=enc_quarter.transform(df[['quarter']]).toarray(), columns=enc_quarter.get_feature_names(['quarter']), dtype=bool)\n",
    "    hot_quarter = hot_quarter * 1\n",
    "    # data for prediction\n",
    "    X = pd.concat((df[['balcony', 'cellar', 'garden', 'lift', 'livingSpace', 'numberOfRooms']], hot_city, hot_quarter), axis=1)\n",
    "    y = df[[\"value\"]]\n",
    "    X = bag_of_words(X, df)\n",
    "    y_pred = model.predict(X)\n",
    "    return y_pred\n",
    "    \n",
    "def bag_of_words(X, df):\n",
    "    # BagOfWords implementation\n",
    "    # As no nice and easy implementation was found this was the next best thing.\n",
    "    # For every word in the vacabulary a column is crated and added if the text includes the corresponding word.\n",
    "    vocabulary = [ \"uni\", \"modern\", \"dach\", \"loft\", \"pool\", \"wg\", \"altbau\", \"luxu\", \"terasse\", \n",
    "                  \"neubau\", \"maisonet\", \"penthouse\", \"erstbezug\", \"kamin\", \"langzeit\", \"renoviert\", \"dachgeschoss\"]\n",
    "    for word in vocabulary:\n",
    "        X[word] = 0\n",
    "        for index, row in df.iterrows():\n",
    "            if word.lower() in row[\"title\"].lower():\n",
    "                #X.set_value(index, word, 1)\n",
    "                X.at[index, word] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rent Analysis\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df2 = df_rent[[\"title\", \"city\", \"quarter\", \"balcony\", \"cellar\",\n",
    "         \"garden\", \"lift\", \"livingSpace\", \"numberOfRooms\", \"value\"]]\n",
    "X, y = encode_data_for_training(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#XGBBoosting -------------------\n",
    "xgb_reg = XGBRegressor(objective ='reg:squarederror')\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7209947784436077\n"
     ]
    }
   ],
   "source": [
    "score = xgb_reg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_reg, 'model_1.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Learning\n",
    "\n",
    "learning with 100% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#max learning with all the data\n",
    "xgb_reg_max = XGBRegressor(objective ='reg:squarederror')\n",
    "xgb_reg_max.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Max Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_reg_max, 'model_1_max.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997453215629898\n"
     ]
    }
   ],
   "source": [
    "score = xgb_reg_max.score(X, y)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buy Analysis\n",
    "## load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=nan, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'model_1_max.sav'\n",
    "loaded_model = joblib.load(filename)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df2 = df_buy[[\"title\", \"city\", \"quarter\", \"balcony\", \"cellar\",\n",
    "         \"garden\", \"lift\", \"livingSpace\", \"numberOfRooms\", \"value\"]]\n",
    "\n",
    "encoder_city = joblib.load('encoder_city.joblib')\n",
    "encoder_quarter = joblib.load('encoder_quarter.joblib')\n",
    "\n",
    "y_pred = predict_data(df2, xgb_reg, encoder_city, encoder_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_buy\n",
    "df_output['prediction'] = y_pred\n",
    "df_output.to_csv(\"data_predict.xls\", sep=';', decimal=\",\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
